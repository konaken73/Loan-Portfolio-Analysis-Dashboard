# etl/pipeline.py
"""
Pipeline ETL pour l'analyse de portefeuille de pr√™ts
Orchestre l'extraction, transformation et chargement des donn√©es
"""

import logging
from datetime import datetime
import os
import sys
import yaml

# Ajouter le r√©pertoire parent au path pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Imports des modules ETL
from etl.extract import DataExtractor
from etl.transform import DataTransformer
from etl.load import DataLoader

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/etl_pipeline.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class LoanETLPipeline:
    """
    Classe principale pour le pipeline ETL
    """
    
    def __init__(self, config_path=None):
        """
        Initialise le pipeline avec configuration
        
        Args:
            config_path: Chemin vers le fichier de configuration YAML
        """
        # Charger la configuration
        self.config = self._load_config(config_path)
        
        # Initialiser les composants
        self.extractor = DataExtractor(self.config['raw_data_path'])
        self.transformer = DataTransformer()
        self.loader = DataLoader(self.config['database_path'])
        
        # Cr√©ation des r√©pertoires n√©cessaires
        self._create_directories()
        
        logger.info("Pipeline ETL initialis√©")
    
    def _load_config(self, config_path):
        """Charge la configuration depuis un fichier YAML"""
        default_config = {
            'raw_data_path': 'data/raw',
            'processed_data_path': 'data/processed',
            'database_path': 'data/loans.db',
            'sample_size': None,  # None pour toutes les donn√©es
            'chunk_size': 100000,
            'outputs_path': 'data/outputs',
            'logs_path': 'logs',
            'reports_path': 'reports',
            'etl_settings': {
                'handle_missing': True,
                'convert_dates': True,
                'create_features': True,
                'remove_outliers': False,
                'outlier_threshold': 3.0
            }
        }
        
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    user_config = yaml.safe_load(f)
                
                # Fusionner avec la configuration par d√©faut
                for key, value in user_config.items():
                    if isinstance(value, dict) and key in default_config:
                        default_config[key].update(value)
                    else:
                        default_config[key] = value
                
                logger.info(f"Configuration charg√©e depuis {config_path}")
                
            except Exception as e:
                logger.warning(f"Erreur de chargement de la configuration: {e}. Utilisation des valeurs par d√©faut.")
        
        return default_config
    
    def _create_directories(self):
        """Cr√©e les r√©pertoires n√©cessaires"""
        directories = [
            self.config['raw_data_path'],
            self.config['processed_data_path'],
            self.config['outputs_path'],
            self.config['logs_path'],
            self.config['reports_path']
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            logger.debug(f"R√©pertoire cr√©√©/v√©rifi√©: {directory}")
    
    def extract(self):
        """
        √âtape 1: Extraction des donn√©es
        
        Returns:
            DataFrame avec les donn√©es brutes
        """
        logger.info("=" * 60)
        logger.info("√âTAPE 1: EXTRACTION")
        logger.info("=" * 60)
        
        try:
            # Option 1: T√©l√©chargement automatique (n√©cessite token Kaggle)
            # files = self.extractor.download_from_kaggle()
            
            # Option 2: Chargement depuis fichiers existants
            raw_files = list(self.extractor.raw_data_dir.glob('*.csv'))
            
            input(raw_files)
            if not raw_files:
                error_msg = f"Aucun fichier CSV trouv√© dans {self.config['raw_data_path']}"
                logger.error(error_msg)
                logger.info("""
                Instructions:
                1. T√©l√©chargez manuellement le dataset Lending Club depuis:
                   https://www.kaggle.com/datasets/wordsforthewise/lending-club
                2. Placez le fichier CSV dans le dossier data/raw/
                3. Relancez le pipeline
                """)
                raise FileNotFoundError(error_msg)
            
            logger.info(f"Fichiers bruts d√©tect√©s: {[f.name for f in raw_files]}")
            
            # Charger les donn√©es
            df_raw = self.extractor.load_raw_data(
                sample_size=self.config['sample_size'],
                chunk_size=self.config['chunk_size']
            )
            
            # Valider les donn√©es brutes
            validation = self.extractor.validate_raw_data(df_raw)
            
            if validation['status'] == 'FAIL':
                logger.warning("Probl√®mes d√©tect√©s dans les donn√©es brutes:")
                for issue in validation['issues']:
                    logger.warning(f"  - {issue}")
            
            # Sauvegarde interm√©diaire
            raw_sample_path = os.path.join(
                self.config['processed_data_path'], 
                'raw_data_sample.csv'
            )
            df_raw.head(1000).to_csv(raw_sample_path, index=False)
            logger.info(f"√âchantillon brut sauvegard√©: {raw_sample_path}")
            
            self.extract_stats = {
                'rows': len(df_raw),
                'columns': len(df_raw.columns),
                'file_count': len(raw_files),
                'validation_status': validation['status']
            }
            
            logger.info(f"Extraction termin√©e: {len(df_raw)} lignes, {len(df_raw.columns)} colonnes")
            
            return df_raw
            
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction: {e}")
            raise
    
    def transform(self, df_raw):
        """
        √âtape 2: Transformation des donn√©es
        
        Args:
            df_raw: DataFrame avec les donn√©es brutes
            
        Returns:
            DataFrame transform√©
        """
        logger.info("=" * 60)
        logger.info("√âTAPE 2: TRANSFORMATION")
        logger.info("=" * 60)
        
        try:
            # Nettoyer et transformer les donn√©es
            df_clean = self.transformer.clean_loan_data(
                df_raw, 
                config=self.config['etl_settings']
            )
            
            # Valider les donn√©es transform√©es
            validation = self.transformer._validate_cleaned_data(df_clean)
            
            # G√©n√©rer le rapport de qualit√©
            self._generate_quality_report(validation)
            
            # S√©lectionner les colonnes pertinentes
            relevant_columns = self._get_relevant_columns(df_clean)
            df_transformed = df_clean[relevant_columns]
            
            # Sauvegarde interm√©diaire
            processed_path = os.path.join(
                self.config['processed_data_path'], 
                'cleaned_data.csv'
            )
            df_transformed.to_csv(processed_path, index=False)
            logger.info(f"Donn√©es nettoy√©es sauvegard√©es: {processed_path}")
            
            self.transform_stats = {
                'rows': len(df_transformed),
                'columns': len(df_transformed.columns),
                'missing_values': validation['stats']['missing_values'],
                'duplicate_rows': validation['stats']['duplicate_rows'],
                'validation_status': validation['status']
            }
            
            logger.info(f"Transformation termin√©e: {len(df_transformed)} lignes, {len(df_transformed.columns)} colonnes")
            
            return df_transformed
            
        except Exception as e:
            logger.error(f"Erreur lors de la transformation: {e}")
            raise
    
    def load(self, df_transformed):
        """
        √âtape 3: Chargement des donn√©es
        
        Args:
            df_transformed: DataFrame transform√©
            
        Returns:
            Tuple (engine, stats)
        """
        logger.info("=" * 60)
        logger.info("√âTAPE 3: CHARGEMENT")
        logger.info("=" * 60)
        
        try:
            # Charger dans la base de donn√©es
            success = self.loader.load_to_sqlite(
                df_transformed, 
                table_name='loans'
            )
            
            if not success:
                raise Exception("√âchec du chargement dans la base de donn√©es")
            
            # Cr√©er les vues analytiques
            self.loader.create_analytical_views()
            
            # Exporter des tables pour le dashboard
            self._export_for_dashboard()
            
            # G√©n√©rer des statistiques
            stats = self.loader.get_database_stats()
            
            self.load_stats = {
                'database_path': self.config['database_path'],
                'table_count': stats.get('table_count', 0),
                'view_count': stats.get('view_count', 0),
                'database_size_mb': stats.get('database_size_mb', 0),
                'loans_row_count': stats.get('loans_row_count', 0)
            }
            
            logger.info(f"Chargement termin√©. Base de donn√©es: {self.config['database_path']}")
            
            return self.loader.engine, stats
            
        except Exception as e:
            logger.error(f"Erreur lors du chargement: {e}")
            raise
    
    def run(self):
        """
        Ex√©cute le pipeline ETL complet
        
        Returns:
            Dict avec les r√©sultats de l'ex√©cution
        """
        logger.info("=" * 60)
        logger.info("D√âMARRAGE DU PIPELINE ETL COMPLET")
        logger.info(f"Date et heure: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 60)
        
        start_time = datetime.now()
        
        try:
            # √âtape 1: Extraction
            df_raw = self.extract()
            
            # √âtape 2: Transformation
            df_transformed = self.transform(df_raw)
            
            # √âtape 3: Chargement
            engine, db_stats = self.load(df_transformed)
            
            # Calcul du temps d'ex√©cution
            end_time = datetime.now()
            duration = end_time - start_time
            
            # G√©n√©rer le rapport final
            self._generate_final_report(start_time, end_time, db_stats)
            
            result = {
                'status': 'success',
                'duration': str(duration),
                'total_rows_processed': len(df_transformed),
                'database_path': self.config['database_path'],
                'extract_stats': self.extract_stats,
                'transform_stats': self.transform_stats,
                'load_stats': self.load_stats,
                'timestamp': end_time.isoformat()
            }
            
            logger.info("=" * 60)
            logger.info("PIPELINE ETL TERMIN√â AVEC SUCC√àS")
            logger.info(f"Dur√©e totale: {duration}")
            logger.info(f"Lignes trait√©es: {len(df_transformed)}")
            logger.info(f"Base de donn√©es: {self.config['database_path']}")
            logger.info("=" * 60)
            
            return result
            
        except Exception as e:
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.error("=" * 60)
            logger.error("√âCHEC DU PIPELINE ETL")
            logger.error(f"Erreur: {e}")
            logger.error(f"Dur√©e avant √©chec: {duration}")
            logger.error("=" * 60)
            
            return {
                'status': 'failed',
                'error': str(e),
                'duration': str(duration),
                'timestamp': end_time.isoformat()
            }
    
    def _get_relevant_columns(self, df):
        """
        D√©finit les colonnes pertinentes pour l'analyse
        """
        # Colonnes de base requises
        base_columns = [
            'id', 'loan_amnt', 'funded_amnt', 'term', 'int_rate',
            'installment', 'grade', 'sub_grade', 'emp_title', 'emp_length',
            'home_ownership', 'annual_inc', 'verification_status',
            'issue_d', 'loan_status', 'purpose', 'title', 'addr_state',
            'dti', 'delinq_2yrs', 'earliest_cr_line', 'inq_last_6mths',
            'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc',
            'initial_list_status', 'out_prncp', 'out_prncp_inv',
            'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',
            'total_rec_int', 'last_pymnt_d', 'last_pymnt_amnt',
            'next_pymnt_d', 'last_credit_pull_d'
        ]
        
        # Colonnes calcul√©es
        calculated_columns = [
            'is_default', 'is_fully_paid', 'income_category',
            'loan_to_income_ratio', 'credit_age_years', 'credit_age_category',
            'risk_category', 'issue_year', 'issue_month', 'issue_quarter',
            'issue_season', 'int_rate_category'
        ]
        
        # Filtrer pour garder seulement les colonnes qui existent
        existing_base = [col for col in base_columns if col in df.columns]
        existing_calculated = [col for col in calculated_columns if col in df.columns]
        
        return existing_base + existing_calculated
    
    def _generate_quality_report(self, validation_report):
        """G√©n√®re un rapport de qualit√© des donn√©es"""
        report_path = os.path.join(self.config['reports_path'], 'data_quality_report.txt')
        
        with open(report_path, 'w') as f:
            f.write("RAPPORT DE QUALIT√â DES DONN√âES\n")
            f.write("=" * 50 + "\n")
            f.write(f"Date de g√©n√©ration: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Pipeline: {self.__class__.__name__}\n\n")
            
            f.write("STATISTIQUES DE BASE:\n")
            f.write("-" * 30 + "\n")
            for key, value in validation_report['stats'].items():
                f.write(f"{key.replace('_', ' ').title()}: {value}\n")
            
            f.write(f"\nSTATUT DE VALIDATION: {validation_report['status']}\n")
            
            if validation_report['issues']:
                f.write("\nPROBL√àMES IDENTIFI√âS:\n")
                f.write("-" * 30 + "\n")
                for issue in validation_report['issues']:
                    f.write(f"‚Ä¢ {issue}\n")
            
            f.write("\nRECOMMANDATIONS:\n")
            f.write("-" * 30 + "\n")
            if validation_report['status'] == 'FAIL':
                f.write("1. V√©rifier la source des donn√©es\n")
                f.write("2. Corriger les probl√®mes list√©s ci-dessus\n")
                f.write("3. Relancer le pipeline apr√®s corrections\n")
            else:
                f.write("‚úì Les donn√©es sont de bonne qualit√©\n")
                f.write("‚úì Le pipeline peut continuer normalement\n")
        
        logger.info(f"Rapport de qualit√© g√©n√©r√©: {report_path}")
    
    def _export_for_dashboard(self):
        """Exporte les donn√©es pour le dashboard"""
        try:
            outputs_dir = Path(self.config['outputs_path'])
            outputs_dir.mkdir(exist_ok=True)
            
            # Tables √† exporter pour le dashboard
            tables_to_export = [
                'dashboard_kpis',
                'loan_default_analysis',
                'monthly_performance',
                'borrower_segmentation',
                'loan_recovery_analysis'
            ]
            
            for table in tables_to_export:
                output_path = outputs_dir / f"{table}.csv"
                self.loader.export_table_to_csv(table, str(output_path))
            
            logger.info(f"Donn√©es export√©es pour le dashboard dans {outputs_dir}")
            
        except Exception as e:
            logger.warning(f"Impossible d'exporter pour le dashboard: {e}")
    
    def _generate_final_report(self, start_time, end_time, db_stats):
        """G√©n√®re un rapport final d'ex√©cution"""
        report_path = os.path.join(self.config['reports_path'], 'etl_execution_report.txt')
        
        with open(report_path, 'w') as f:
            f.write("RAPPORT D'EX√âCUTION ETL\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("INFORMATIONS G√âN√âRALES:\n")
            f.write("-" * 30 + "\n")
            f.write(f"Date de d√©but: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Date de fin: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Dur√©e totale: {end_time - start_time}\n")
            f.write(f"Statut: SUCC√àS\n\n")
            
            f.write("STATISTIQUES D'EXTRACTION:\n")
            f.write("-" * 30 + "\n")
            if hasattr(self, 'extract_stats'):
                for key, value in self.extract_stats.items():
                    f.write(f"{key.replace('_', ' ').title()}: {value}\n")
            
            f.write("\nSTATISTIQUES DE TRANSFORMATION:\n")
            f.write("-" * 30 + "\n")
            if hasattr(self, 'transform_stats'):
                for key, value in self.transform_stats.items():
                    f.write(f"{key.replace('_', ' ').title()}: {value}\n")
            
            f.write("\nSTATISTIQUES DE CHARGEMENT:\n")
            f.write("-" * 30 + "\n")
            if hasattr(self, 'load_stats'):
                for key, value in self.load_stats.items():
                    f.write(f"{key.replace('_', ' ').title()}: {value}\n")
            
            f.write("\nCONFIGURATION UTILIS√âE:\n")
            f.write("-" * 30 + "\n")
            for key, value in self.config.items():
                if key != 'etl_settings':
                    f.write(f"{key}: {value}\n")
            
            f.write("\nPARAM√àTRES ETL:\n")
            f.write("-" * 30 + "\n")
            for key, value in self.config['etl_settings'].items():
                f.write(f"{key}: {value}\n")
            
            f.write("\nRECOMMANDATIONS POUR LA PROCHAINE EX√âCUTION:\n")
            f.write("-" * 30 + "\n")
            f.write("1. V√©rifier la fra√Æcheur des donn√©es sources\n")
            f.write("2. Mettre √† jour les param√®tres si n√©cessaire\n")
            f.write("3. Planifier une ex√©cution r√©guli√®re\n")
            f.write("4. Surveiller les logs pour d√©tecter les anomalies\n")
        
        logger.info(f"Rapport d'ex√©cution g√©n√©r√©: {report_path}")


def run_pipeline(config_path=None):
    """
    Fonction principale pour ex√©cuter le pipeline
    
    Args:
        config_path: Chemin vers le fichier de configuration
        
    Returns:
        R√©sultats de l'ex√©cution
    """
    try:
        # Cr√©ation et ex√©cution du pipeline
        pipeline = LoanETLPipeline(config_path)
        result = pipeline.run()
        
        return result
        
    except Exception as e:
        logger.error(f"Erreur lors de l'ex√©cution du pipeline: {e}")
        return {
            'status': 'failed',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }


if __name__ == "__main__":
    # Point d'entr√©e principal
    import argparse
    
    parser = argparse.ArgumentParser(description='Ex√©cute le pipeline ETL pour les donn√©es de pr√™t')
    parser.add_argument('--config', type=str, help='Chemin vers le fichier de configuration YAML')
    parser.add_argument('--sample-size', type=int, help='Taille de l\'√©chantillon √† charger')
    parser.add_argument('--db-path', type=str, help='Chemin de la base de donn√©es de sortie')
    
    args = parser.parse_args()
    
    # Configuration personnalis√©e
    custom_config = {}
    
    if args.config:
        custom_config['config_path'] = args.config
    
    if args.sample_size:
        if 'config' not in custom_config:
            custom_config['config'] = {}
        custom_config['config']['sample_size'] = args.sample_size
    
    if args.db_path:
        if 'config' not in custom_config:
            custom_config['config'] = {}
        custom_config['config']['database_path'] = args.db_path
    
    # Ex√©cuter le pipeline
    result = run_pipeline(
        config_path=custom_config.get('config_path') if custom_config else None
    )
    
    # Afficher le r√©sultat
    if result['status'] == 'success':
        print("\n" + "="*60)
        print("‚úÖ PIPELINE EX√âCUT√â AVEC SUCC√àS!")
        print("="*60)
        print(f"üìä Dur√©e: {result['duration']}")
        print(f"üìà Lignes trait√©es: {result['total_rows_processed']:,}")
        print(f"üíæ Base de donn√©es: {result['database_path']}")
        print(f"üïí Timestamp: {result['timestamp']}")
        print("="*60 + "\n")
    else:
        print(f"\n‚ùå √âchec du pipeline: {result.get('error', 'Erreur inconnue')}\n")
