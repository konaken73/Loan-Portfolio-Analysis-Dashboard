#!/usr/bin/env python3
"""
Script d'ex√©cution s√©curis√© du pipeline ETL
√âvite l'erreur "too many SQL variables"
"""

import logging
import sys
import os
from etl.pipeline import LoanETLPipeline

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def run_safe_etl():
    """Ex√©cute le pipeline ETL avec des param√®tres s√©curis√©s"""
    
    print("""
    ========================================
    EX√âCUTION S√âCURIS√âE DU PIPELINE ETL
    ========================================
    
    Ce script utilise des param√®tres optimis√©s pour √©viter
    l'erreur "too many SQL variables" de SQLite.
    """)
    
    # Demander √† l'utilisateur
    use_sample = input("Utiliser un √©chantillon? (o/n) [o]: ").lower() != 'n'
    
    if use_sample:
        sample_size = input("Taille de l'√©chantillon (ex: 10000) [10000]: ")
        sample_size = int(sample_size) if sample_size else 10000
    else:
        sample_size = None
    
    # Configuration s√©curis√©e
    config = {
        'raw_data_path': 'data/raw',
        'processed_data_path': 'data/processed',
        'database_path': 'data/loans_safe.db',
        'sample_size': sample_size,
        'chunk_size': 10000,
        'batch_size': 300,  # R√©duit pour SQLite
        'outputs_path': 'data/outputs',
        'logs_path': 'logs',
        'reports_path': 'reports',
        'etl_settings': {
            'handle_missing': True,
            'convert_dates': True,
            'create_features': True,
            'remove_outliers': False,
            'outlier_threshold': 3.0,
            'max_columns': 25  # Limite le nombre de colonnes
        }
    }
    
    print(f"\nConfiguration:")
    print(f"- Sample size: {sample_size if sample_size else 'Toutes les donn√©es'}")
    print(f"- Batch size: {config['batch_size']}")
    print(f"- Max columns: {config['etl_settings']['max_columns']}")
    print(f"- Database: {config['database_path']}")
    
    confirm = input("\nConfirmer l'ex√©cution? (o/n) [o]: ").lower()
    if confirm == 'n':
        print("Annulation.")
        return
    
    # Ex√©cuter le pipeline
    try:
        # Cr√©er une instance avec configuration personnalis√©e
        class SafePipeline(LoanETLPipeline):
            def _get_relevant_columns(self, df):
                """Version r√©duite pour √©viter l'erreur SQLite"""
                essential_columns = [
                    'id', 'loan_amnt', 'int_rate', 'term', 'grade',
                    'sub_grade', 'issue_d', 'loan_status', 'annual_inc',
                    'dti', 'home_ownership', 'emp_length', 'verification_status',
                    'purpose', 'addr_state', 'delinq_2yrs', 'earliest_cr_line',
                    'inq_last_6mths', 'open_acc', 'revol_bal', 'revol_util',
                    'total_acc', 'total_pymnt', 'last_pymnt_d', 'last_pymnt_amnt'
                ]
                
                # Colonnes calcul√©es
                calculated_columns = [
                    'is_default', 'is_fully_paid', 'income_category',
                    'risk_category'
                ]
                
                # Prendre seulement les colonnes qui existent
                existing_essential = [col for col in essential_columns if col in df.columns]
                existing_calculated = [col for col in calculated_columns if col in df.columns]
                
                # Limiter √† 25 colonnes
                all_columns = existing_essential + existing_calculated
                if len(all_columns) > 25:
                    logger.warning(f"Limitation √† 25 colonnes sur {len(all_columns)}")
                    all_columns = all_columns[:25]
                
                return all_columns
        
        pipeline = SafePipeline()
        
        # Modifier la configuration
        for key, value in config.items():
            if key in pipeline.config:
                if isinstance(value, dict) and isinstance(pipeline.config[key], dict):
                    pipeline.config[key].update(value)
                else:
                    pipeline.config[key] = value
        
        # Ex√©cuter
        result = pipeline.run()
        
        if result['status'] == 'success':
            print("\n" + "="*60)
            print("‚úÖ PIPELINE EX√âCUT√â AVEC SUCC√àS!")
            print("="*60)
            print(f"Lignes trait√©es: {result.get('total_rows_processed', 0):,}")
            print(f"Base de donn√©es: {result.get('database_path', 'N/A')}")
            print(f"Dur√©e: {result.get('duration', 'N/A')}")
            print("="*60)
            
            # Conseils pour la suite
            print("\nüìä Pour cr√©er le dashboard:")
            print("1. python scripts/export_for_bi.py")
            print("2. Importer les fichiers CSV de data/outputs/ dans Looker Studio")
            
        else:
            print(f"\n‚ùå √âchec du pipeline: {result.get('error', 'Erreur inconnue')}")
            
    except Exception as e:
        print(f"\n‚ùå Erreur critique: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    run_safe_etl()
